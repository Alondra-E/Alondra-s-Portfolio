# -*- coding: utf-8 -*-
"""Blue_Bottle_Coffee_code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JU-VbTJCd-OzJUUgLV59eu1I-Q2paq5r

# Step 1: Read and preprocess the data
"""

!pip install kaggle

# Download and unzip the dataset to the specified folder
!kaggle datasets download -d mrsimple07/salary-prediction-data --unzip -p /content/dataset

import os

# List files in the dataset folder to verify
print(os.listdir("/content/dataset"))

import pandas as pd

# Adjust this path with the correct filename from the directory
file_path = "/content/dataset/salary_prediction_data.csv"
df = pd.read_csv(file_path)
print(df.head())

print(df.shape)

df.columns

# Summary statistics for numeric columns
print(df.describe())

df.dtypes

df.isnull().sum()

# Value counts for categorical columns
print(df['Job_Title'].value_counts())
print(df['Location'].value_counts())

# Convert float columns to int64
df['Salary'] = df['Salary'].astype('int64')
print(df.dtypes)

df.drop('Experience', axis=1, inplace=True)
df.columns

import pandas as pd
import numpy as np

# Create a new column for Salary with standardized labels
df['Salary_Category'] = np.where(df['Salary'] >= 100000, '>=100k', '<100k')
print(df.head())

# Check the result to ensure uniform labels
# print(df[['Salary', 'Salary_Category']].head())

df.head(20)

df['Salary_Category'].value_counts()

import pandas as pd
import matplotlib.pyplot as plt

# Only rows where Salary_Category is '>=100k'
high_salary_data = df[df['Salary_Category'] == '>=100k']

# Number of people with >=100k in each Location
location_counts = high_salary_data['Location'].value_counts()

# Plotting
plt.figure(figsize=(8, 6))
location_counts.plot(kind='bar', color=['lightskyblue', 'lightblue', 'skyblue'])
plt.title("Number of People with Salary >= 100k by Location")
plt.xlabel("Location")
plt.ylabel("Number of People")
plt.xticks(rotation=0)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Only rows where Salary_Category is '>=100k'
high_salary = df[df['Salary_Category'] == '>=100k']

# Number of people with >=100k in each Location
location_counts = high_salary_data['Location'].value_counts()

# Plotting the pie chart
plt.figure(figsize=(8, 6))
location_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['lightskyblue', 'lightblue', 'skyblue'])
plt.title("Percentage of People with Salary >= 100k by Location")
plt.show()

# Separate predictors (features) and target variable
X = df.drop('Salary_Category', axis=1)

# Select categorical columns for creating dummy variables
categorical_columns = ['Location', 'Education', 'Job_Title', 'Age', 'Gender']
X_categorical = X[categorical_columns]

# Create dummy variables for categorical predictors
X = pd.get_dummies(X_categorical, drop_first=True)
X

#Encoding target variable
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
y = labelencoder.fit_transform(df['Salary_Category'])
y

"""# Step 2: Perform Analysis Techniques

# Decision Tree

Decision Tree Model
"""

#import libraries needed for analyses
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

X.head(10)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

# Decision tree classifier
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(random_state = 0, criterion="entropy")
tree.fit(X_train, y_train)

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(tree, X_train, y_train)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Get predictions and confusion matrix
y_pred = tree.predict(X_train)
cm = confusion_matrix(y_train, y_pred)

# Evaluate Decision Tree Model on Test set
y_pred = tree.predict(X_test)

from sklearn.metrics import accuracy_score
print('Accuracy on training set:',tree.score(X_train, y_train))
print("Accuracy on test set:",accuracy_score(y_pred, y_test))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred, labels=tree.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=tree.classes_)
disp.plot()

plt.show()

# Prune tree to address overfitting
tree_pruned = DecisionTreeClassifier(max_depth=4, random_state=0, criterion= "entropy")
tree_pruned.fit(X_train, y_train)
y_pruned_pred = tree_pruned.predict(X_test)

# Print accuracy on training and test sets with rounded values
print("Accuracy on training set:", round(tree_pruned.score(X_train, y_train), 3))
print("Accuracy on test set:", round(accuracy_score(y_pruned_pred, y_test), 3))

# variable importance in decision tree
importance = pd.DataFrame(tree_pruned.feature_importances_, index = X.columns, columns = ["Importance"])
importance_sorted = importance.sort_values(by = "Importance", ascending = False)
importance_sorted

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(60,20))
fig.patch.set_facecolor('#212121')
display_tree = plot_tree(tree_pruned,
              feature_names=X.columns,
              class_names=["under 100k", "over 100k"],
              filled=True,
              rounded=True,
              fontsize=20)

plt.savefig('decision_tree.png', format='png', dpi=300, bbox_inches='tight')

"""# Bagging"""

from sklearn.ensemble import BaggingClassifier

bagging = BaggingClassifier(random_state=0)
bagging.fit(X_train, y_train)
y_bagging_pred = bagging.predict(X_test)

print("Bagging Model Accuracy on test set: {:.3f}".format(accuracy_score(y_test,y_bagging_pred)))

"""# Boosted model"""

from sklearn.ensemble import AdaBoostClassifier

boost = AdaBoostClassifier(n_estimators=10, random_state=0)
boost.fit(X_train, y_train)

y_boost_pred = boost.predict(X_test)
print("Boosted Model Accuracy on test set: {:.3f}".format(accuracy_score(y_boost_pred, y_test)))

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators=10, random_state=0)
forest.fit(X_train, y_train)

y_rf_pred = forest.predict(X_test)
print("Random Forest Accuracy on test set: {:.3f}".format(accuracy_score(y_test, y_rf_pred)))

"""# Logistic Regression"""

from sklearn import preprocessing
scaler = preprocessing.StandardScaler()
X_standardized = scaler.fit_transform(X)

X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_standardized, y, test_size=0.1, random_state=0)

from sklearn.linear_model import LogisticRegression

# instantiate the model (using the default parameters)
logreg = LogisticRegression()

# fit the model with training data
logreg.fit(X_train_lr,y_train_lr)

# Interpret coefficient, which is saved in .coef attribute
coef = pd.DataFrame(logreg.coef_[0], index = X.columns,columns=['Coefficients'])
coef

# coefficient is intepreted as with one unit change in X, how many unit will change in log(odds). We need to take exponentiate
# of the original coeffient and interpret it in terms of odds
import numpy as np
df = pd.DataFrame(np.exp(logreg.coef_[0]), index = X.columns,columns = ['Odds'])
df
df.sort_values(by=['Odds'], ascending = False)

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(logreg, X_train_lr, y_train_lr)

y_pred_lr = logreg.predict(X_test_lr)
y_pred_lr

#accuracy as one evaluation metrics
from sklearn import metrics
print("Accuracy on the training set: {:.3f}".format(logreg.score(X_train_lr, y_train_lr)))
print("Accuracy on the test set: {:.3f}".format(metrics.accuracy_score(y_test_lr, y_pred_lr)))

# interpret confuison matrix
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

cm = confusion_matrix(y_test_lr, y_pred_lr, labels=logreg.classes_)
dsp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)
dsp.plot()

"""# Cross-Validation"""

from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# Initialize the Logistic Regression model
logreg = LogisticRegression(max_iter=2000, solver='saga', random_state=0)

# Standardize the full dataset if you haven't done so already
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)  # Replace 'X' with your features dataset

# Define KFold cross-validator
cv = KFold(n_splits=10, shuffle=True, random_state=0)

# Perform cross-validation
scores = cross_val_score(logreg, X_standardized, y, cv=cv)  # 'y' is your target variable

# Plotting the cross-validation scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), scores, marker='o', linestyle='-', color='r')
plt.title('10-fold Cross Validation Performance for Logistic Regression')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.xticks(range(1, 11))
plt.grid(True)
plt.show()

# Calculate and print average accuracy
average_score = scores.mean()
print(f"Average Accuracy: {average_score:.3f}")